{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text2phraser import text2phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grams = 5\n",
    "text = '''\n",
    "Unsupervised representation learning is a fairly well studied problem in general computer vision\n",
    "research, as well as in the context of images. A classic approach to unsupervised representation\n",
    "learning is to do clustering on the data (for example using K-means), and leverage the clusters for\n",
    "improved classification scores. In the context of images, one can do hierarchical clustering of image\n",
    "patches (Coates & Ng, 2012) to learn powerful image representations. Another popular method\n",
    "is to train auto-encoders (convolutionally, stacked (Vincent et al., 2010), separating the what and\n",
    "where components of the code (Zhao et al., 2015), ladder structures (Rasmus et al., 2015)) that\n",
    "encode an image into a compact code, and decode the code to reconstruct the image as accurately\n",
    "as possible. These methods have also been shown to learn good feature representations from image\n",
    "pixels. Deep belief networks (Lee et al., 2009) have also been shown to work well in learning\n",
    "hierarchical representations.\n",
    "Generative image models are well studied and fall into two categories: parametric and non-\n",
    "parametric.\n",
    "The non-parametric models often do matching from a database of existing images, often matching\n",
    "patches of images, and have been used in texture synthesis (Efros et al., 1999), super-resolution\n",
    "(Freeman et al., 2002) and in-painting (Hays & Efros, 2007).\n",
    "Parametric models for generating images has been explored extensively (for example on MNIST\n",
    "digits or for texture synthesis (Portilla & Simoncelli, 2000)). However, generating natural images\n",
    "of the real world have had not much success until recently. A variational sampling approach to\n",
    "generating images (Kingma & Welling, 2013) has had some success, but the samples often suffer\n",
    "from being blurry. Another approach generates images using an iterative forward diffusion process\n",
    "(Sohl-Dickstein et al., 2015). Generative Adversarial Networks (Goodfellow et al., 2014) generated\n",
    "images suffering from being noisy and incomprehensible. A laplacian pyramid extension to this\n",
    "approach (Denton et al., 2015) showed higher quality images, but they still suffered from the objects\n",
    "looking wobbly because of noise introduced in chaining multiple models. A recurrent network\n",
    "approach (Gregor et al., 2015) and a deconvolution network approach (Dosovitskiy et al., 2014) have\n",
    "also recently had some success with generating natural images. However, they have not leveraged\n",
    "the generators for supervised tasks.\n",
    "One constant criticism of using neural networks has been that they are black-box methods, with little\n",
    "understanding of what the networks do in the form of a simple human-consumable algorithm. In the\n",
    "context of CNNs, Zeiler et. al. (Zeiler & Fergus, 2014) showed that by using deconvolutions and\n",
    "filtering the maximal activations, one can find the approximate purpose of each convolution filter in\n",
    "the network. Similarly, using a gradient descent on the inputs lets us inspect the ideal image that\n",
    "activates certain subsets of filters (Mordvintsev et al.).\n",
    "\n",
    "Historical attempts to scale up GANs using CNNs to model images have been unsuccessful. This\n",
    "motivated the authors of LAPGAN (Denton et al., 2015) to develop an alternative approach to it-\n",
    "eratively upscale low resolution generated images which can be modeled more reliably. We also\n",
    "encountered difficulties attempting to scale GANs using CNN architectures commonly used in the\n",
    "supervised literature. However, after extensive model exploration we identified a family of archi-\n",
    "2Under review as a conference paper at ICLR 2016\n",
    "tectures that resulted in stable training across a range of datasets and allowed for training higher\n",
    "resolution and deeper generative models.\n",
    "Core to our approach is adopting and modifying three recently demonstrated changes to CNN archi-\n",
    "tectures.\n",
    "The first is the all convolutional net (Springenberg et al., 2014) which replaces deterministic spatial\n",
    "pooling functions (such as maxpooling) with strided convolutions, allowing the network to learn\n",
    "its own spatial downsampling. We use this approach in our generator, allowing it to learn its own\n",
    "spatial upsampling, and discriminator.\n",
    "Second is the trend towards eliminating fully connected layers on top of convolutional features.\n",
    "The strongest example of this is global average pooling which has been utilized in state of the\n",
    "art image classification models (Mordvintsev et al.). We found global average pooling increased\n",
    "model stability but hurt convergence speed. A middle ground of directly connecting the highest\n",
    "convolutional features to the input and output respectively of the generator and discriminator worked\n",
    "well. The first layer of the GAN, which takes a uniform noise distribution Z as input, could be called\n",
    "fully connected as it is just a matrix multiplication, but the result is reshaped into a 4-dimensional\n",
    "tensor and used as the start of the convolution stack. For the discriminator, the last convolution layer\n",
    "is flattened and then fed into a single sigmoid output. See Fig. 1 for a visualization of an example\n",
    "model architecture.\n",
    "Third is Batch Normalization (Ioffe & Szegedy, 2015) which stabilizes learning by normalizing the\n",
    "input to each unit to have zero mean and unit variance. This helps deal with training problems that\n",
    "arise due to poor initialization and helps gradient flow in deeper models. This proved critical to get\n",
    "deep generators to begin learning, preventing the generator from collapsing all samples to a single\n",
    "point which is a common failure mode observed in GANs. Directly applying batchnorm to all layers\n",
    "however, resulted in sample oscillation and model instability. This was avoided by not applying\n",
    "batchnorm to the generator output layer and the discriminator input layer.\n",
    "The ReLU activation (Nair & Hinton, 2010) is used in the generator with the exception of the output\n",
    "layer which uses the Tanh function. We observed that using a bounded activation allowed the model\n",
    "to learn more quickly to saturate and cover the color space of the training distribution. Within the\n",
    "discriminator we found the leaky rectified activation (Maas et al., 2013) (Xu et al., 2015) to work\n",
    "well, especially for higher resolution modeling. This is in contrast to the original GAN paper, which\n",
    "used the maxout activation (Goodfellow et al., 2013)\n",
    "'''\n",
    "text = text.replace(\"\\n\", \" \").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning of phrasing phase\n",
      "2-phraser:\n",
      "['computer_vision', 'hierarchical_clustering', 'has_been', 'real_world', 'neural_networks', 'has_been', 'gradient_descent', 'commonly_used', 'its_own', 'its_own', 'fully_connected', 'has_been', 'fully_connected']\n",
      "3-phraser:\n",
      "['computer_vision', 'hierarchical_clustering', 'image_patches', 'texture_synthesis', 'has_been', 'texture_synthesis', 'real_world', 'recurrent_network', 'neural_networks', 'has_been', 'gradient_descent', 'lets_us', 'low_resolution', 'commonly_used', 'convolutional_net', 'spatial_pooling', 'its_own', 'its_own', 'trend_towards', 'fully_connected_layers', 'has_been', 'fully_connected', 'fed_into', 'deal_with', 'failure_mode']\n",
      "4-phraser:\n",
      "['computer_vision', 'hierarchical_clustering', 'image_patches', 'feature_representations', 'belief_networks', 'texture_synthesis', 'has_been', 'texture_synthesis', 'natural_images', 'real_world', 'diffusion_process', 'recurrent_network', 'neural_networks', 'has_been', 'gradient_descent', 'lets_us', 'low_resolution', 'commonly_used', 'higher_resolution', 'convolutional_net', 'spatial_pooling', 'its_own', 'its_own', 'trend_towards', 'fully_connected_layers', 'has_been', 'fully_connected', 'fed_into', 'deal_with', 'failure_mode', 'higher_resolution']\n",
      "5-phraser:\n",
      "['computer_vision', 'hierarchical_clustering', 'image_patches', 'feature_representations', 'belief_networks', 'texture_synthesis', 'has_been', 'texture_synthesis', 'natural_images', 'real_world', 'diffusion_process', 'recurrent_network', 'neural_networks', 'has_been', 'gradient_descent', 'lets_us', 'low_resolution', 'commonly_used', 'higher_resolution', 'convolutional_net', 'spatial_pooling', 'its_own', 'its_own', 'trend_towards', 'fully_connected_layers', 'has_been', 'fully_connected', 'fed_into', 'deal_with', 'failure_mode', 'higher_resolution']\n",
      "Phrase of 5-grams computed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Unsupervised representation learning is a fairly well studied problem in general computer_vision research, as well as in the context of images. A classic approach to unsupervised representation learning is to do clustering on the data (for example using K-means), and leverage the clusters for improved classification scores. In the context of images, one can do hierarchical_clustering of image_patches (Coates & Ng, 2012) to learn powerful image representations. Another popular method is to train auto-encoders (convolutionally, stacked (Vincent et al., 2010), separating the what and where components of the code (Zhao et al., 2015), ladder structures (Rasmus et al., 2015)) that encode an image into a compact code, and decode the code to reconstruct the image as accurately as possible. These methods have also been shown to learn good feature_representations from image pixels. Deep belief_networks (Lee et al., 2009) have also been shown to work well in learning hierarchical representations. Generative image models are well studied and fall into two categories: parametric and non- parametric. The non-parametric models often do matching from a database of existing images, often matching patches of images, and have been used in texture_synthesis (Efros et al., 1999), super-resolution (Freeman et al., 2002) and in-painting (Hays & Efros, 2007). Parametric models for generating images has_been explored extensively (for example on MNIST digits or for texture_synthesis (Portilla & Simoncelli, 2000)). However, generating natural_images of the real_world have had not much success until recently. A variational sampling approach to generating images (Kingma & Welling, 2013) has had some success, but the samples often suffer from being blurry. Another approach generates images using an iterative forward diffusion_process (Sohl-Dickstein et al., 2015). Generative Adversarial Networks (Goodfellow et al., 2014) generated images suffering from being noisy and incomprehensible. A laplacian pyramid extension to this approach (Denton et al., 2015) showed higher quality images, but they still suffered from the objects looking wobbly because of noise introduced in chaining multiple models. A recurrent_network approach (Gregor et al., 2015) and a deconvolution network approach (Dosovitskiy et al., 2014) have also recently had some success with generating natural images. However, they have not leveraged the generators for supervised tasks. One constant criticism of using neural_networks has_been that they are black-box methods, with little understanding of what the networks do in the form of a simple human-consumable algorithm. In the context of CNNs, Zeiler et. al. (Zeiler & Fergus, 2014) showed that by using deconvolutions and filtering the maximal activations, one can find the approximate purpose of each convolution filter in the network. Similarly, using a gradient_descent on the inputs lets_us inspect the ideal image that activates certain subsets of filters (Mordvintsev et al.). Historical attempts to scale up GANs using CNNs to model images have been unsuccessful. This motivated the authors of LAPGAN (Denton et al., 2015) to develop an alternative approach to it- eratively upscale low_resolution generated images which can be modeled more reliably. We also encountered difficulties attempting to scale GANs using CNN architectures commonly_used in the supervised literature. However, after extensive model exploration we identified a family of archi- 2Under review as a conference paper at ICLR 2016 tectures that resulted in stable training across a range of datasets and allowed for training higher_resolution and deeper generative models. Core to our approach is adopting and modifying three recently demonstrated changes to CNN archi- tectures. The first is the all convolutional_net (Springenberg et al., 2014) which replaces deterministic spatial_pooling functions (such as maxpooling) with strided convolutions, allowing the network to learn its_own spatial downsampling. We use this approach in our generator, allowing it to learn its_own spatial upsampling, and discriminator. Second is the trend_towards eliminating fully_connected_layers on top of convolutional features. The strongest example of this is global average pooling which has_been utilized in state of the art image classification models (Mordvintsev et al.). We found global average pooling increased model stability but hurt convergence speed. A middle ground of directly connecting the highest convolutional features to the input and output respectively of the generator and discriminator worked well. The first layer of the GAN, which takes a uniform noise distribution Z as input, could be called fully_connected as it is just a matrix multiplication, but the result is reshaped into a 4-dimensional tensor and used as the start of the convolution stack. For the discriminator, the last convolution layer is flattened and then fed_into a single sigmoid output. See Fig. 1 for a visualization of an example model architecture. Third is Batch Normalization (Ioffe & Szegedy, 2015) which stabilizes learning by normalizing the input to each unit to have zero mean and unit variance. This helps deal_with training problems that arise due to poor initialization and helps gradient flow in deeper models. This proved critical to get deep generators to begin learning, preventing the generator from collapsing all samples to a single point which is a common failure_mode observed in GANs. Directly applying batchnorm to all layers however, resulted in sample oscillation and model instability. This was avoided by not applying batchnorm to the generator output layer and the discriminator input layer. The ReLU activation (Nair & Hinton, 2010) is used in the generator with the exception of the output layer which uses the Tanh function. We observed that using a bounded activation allowed the model to learn more quickly to saturate and cover the color space of the training distribution. Within the discriminator we found the leaky rectified activation (Maas et al., 2013) (Xu et al., 2015) to work well, especially for higher_resolution modeling. This is in contrast to the original GAN paper, which used the maxout activation (Goodfellow et al., 2013)'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2phrase(text, \"./model/all_text_transciption_model_07-12-2018/\", n_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"./../../all_text_transcriptions/data_3_30simpozijobdobja2011_stabej_okroglamiza_video_01_i5zrg22kgeaycppl2z3sb4fg5dg3rfo2.en.tx.dfxp.txt\", \"r\") as f:\n",
    "#         text = \"\".join(f.readlines()).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
